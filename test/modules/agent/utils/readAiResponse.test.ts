import { describe, it, expect } from 'vitest'
import { readAiResponse } from '../../../../src/modules/agent/utils/readAiResponse'
import type { ChatCompletion } from '../../../../src/modules/agent/types'

describe('readAiResponse', () => {
  describe('deberÃ­a extraer el contenido de la respuesta correctamente', () => {
    it('deberÃ­a retornar el contenido cuando existe', () => {
      const response: ChatCompletion = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content: 'Esta es una respuesta de prueba',
              refusal: null,
            },
            finish_reason: 'stop',
            logprobs: null,
          },
        ],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      }

      const result = readAiResponse(response)
      expect(result).toBe('Esta es una respuesta de prueba')
    })

    it('deberÃ­a retornar contenido largo correctamente', () => {
      const longContent = 'Lorem ipsum dolor sit amet, '.repeat(50)
      const response: ChatCompletion = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content: longContent,
              refusal: null,
            },
            finish_reason: 'stop',
            logprobs: null,
          },
        ],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      }

      const result = readAiResponse(response)
      expect(result).toBe(longContent)
      expect(result.length).toBeGreaterThan(1000)
    })

    it('deberÃ­a manejar contenido con saltos de lÃ­nea', () => {
      const response: ChatCompletion = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content: 'LÃ­nea 1\nLÃ­nea 2\nLÃ­nea 3',
              refusal: null,
            },
            finish_reason: 'stop',
            logprobs: null,
          },
        ],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      }

      const result = readAiResponse(response)
      expect(result).toBe('LÃ­nea 1\nLÃ­nea 2\nLÃ­nea 3')
    })

    it('deberÃ­a manejar contenido con caracteres especiales', () => {
      const response: ChatCompletion = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content: 'Â¡Hola! Â¿CÃ³mo estÃ¡s? #test @usuario',
              refusal: null,
            },
            finish_reason: 'stop',
            logprobs: null,
          },
        ],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      }

      const result = readAiResponse(response)
      expect(result).toBe('Â¡Hola! Â¿CÃ³mo estÃ¡s? #test @usuario')
    })

    it('deberÃ­a manejar emojis en el contenido', () => {
      const response: ChatCompletion = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content: 'Â¡Hola! ðŸ‘‹ Estoy aquÃ­ para ayudarte ðŸ˜Š',
              refusal: null,
            },
            finish_reason: 'stop',
            logprobs: null,
          },
        ],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      }

      const result = readAiResponse(response)
      expect(result).toBe('Â¡Hola! ðŸ‘‹ Estoy aquÃ­ para ayudarte ðŸ˜Š')
    })

    it('deberÃ­a manejar cÃ³digo en el contenido', () => {
      const response: ChatCompletion = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content: '```typescript\nconst x = 5;\nconsole.log(x);\n```',
              refusal: null,
            },
            finish_reason: 'stop',
            logprobs: null,
          },
        ],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      }

      const result = readAiResponse(response)
      expect(result).toBe('```typescript\nconst x = 5;\nconsole.log(x);\n```')
    })
  })

  describe('deberÃ­a manejar casos donde el contenido es nulo o undefined', () => {
    it('deberÃ­a retornar string vacÃ­o cuando content es null', () => {
      const response: ChatCompletion = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content: null,
              refusal: null,
            },
            finish_reason: 'stop',
            logprobs: null,
          },
        ],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      }

      const result = readAiResponse(response)
      expect(result).toBe('')
    })

    it('deberÃ­a retornar string vacÃ­o cuando content es undefined', () => {
      const response: ChatCompletion = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content: null,
              refusal: null,
            },
            finish_reason: 'stop',
            logprobs: null,
          },
        ],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      }

      const result = readAiResponse(response)
      expect(result).toBe('')
    })

    it('deberÃ­a retornar string vacÃ­o cuando content es string vacÃ­o', () => {
      const response: ChatCompletion = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content: '',
              refusal: null,
            },
            finish_reason: 'stop',
            logprobs: null,
          },
        ],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      }

      const result = readAiResponse(response)
      expect(result).toBe('')
    })

    it('deberÃ­a retornar string vacÃ­o cuando message es undefined', () => {
      const response = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [
          {
            index: 0,
            message: undefined,
            finish_reason: 'stop',
            logprobs: null,
          },
        ],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      } as unknown as ChatCompletion

      const result = readAiResponse(response)
      expect(result).toBe('')
    })

    it('deberÃ­a retornar string vacÃ­o cuando choices[0] es undefined', () => {
      const response: ChatCompletion = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      }

      const result = readAiResponse(response)
      expect(result).toBe('')
    })
  })

  describe('deberÃ­a extraer solo del primer choice', () => {
    it('deberÃ­a ignorar choices adicionales y tomar solo el primero', () => {
      const response: ChatCompletion = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content: 'Primera respuesta',
              refusal: null,
            },
            finish_reason: 'stop',
            logprobs: null,
          },
          {
            index: 1,
            message: {
              role: 'assistant',
              content: 'Segunda respuesta',
              refusal: null,
            },
            finish_reason: 'stop',
            logprobs: null,
          },
        ],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      }

      const result = readAiResponse(response)
      expect(result).toBe('Primera respuesta')
      expect(result).not.toBe('Segunda respuesta')
    })
  })

  describe('deberÃ­a manejar diferentes tipos de finish_reason', () => {
    it('deberÃ­a manejar respuesta con finish_reason "length"', () => {
      const response: ChatCompletion = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content: 'Respuesta truncada por longitud',
              refusal: null,
            },
            finish_reason: 'length',
            logprobs: null,
          },
        ],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      }

      const result = readAiResponse(response)
      expect(result).toBe('Respuesta truncada por longitud')
    })

    it('deberÃ­a manejar respuesta con finish_reason "tool_calls"', () => {
      const response: ChatCompletion = {
        id: 'chatcmpl-123',
        object: 'chat.completion',
        created: 1677652288,
        model: 'gpt-4',
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content: null,
              refusal: null,
              tool_calls: [
                {
                  id: 'call_123',
                  type: 'function',
                  function: {
                    name: 'get_weather',
                    arguments: '{"location": "Madrid"}',
                  },
                },
              ],
            },
            finish_reason: 'tool_calls',
            logprobs: null,
          },
        ],
        usage: {
          prompt_tokens: 10,
          completion_tokens: 20,
          total_tokens: 30,
        },
      }

      const result = readAiResponse(response)
      expect(result).toBe('')
    })
  })
})
